{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text summarization"
      ],
      "metadata": {
        "id": "de3KXS8uQ9qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data (scraping from Wikipedia)"
      ],
      "metadata": {
        "id": "ghgzB9IqRFG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_wikipedia_articles(topic):\n",
        "    \"\"\"\n",
        "    Fetches text from up to 5 Wikipedia articles related to the given topic.\n",
        "\n",
        "    Parameters:\n",
        "    topic (str): The main topic to search articles for.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary where keys are article titles and values are article content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Wikipedia API URL for searching articles\n",
        "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    # Custom headers with User-Agent\n",
        "    headers = {\n",
        "        \"User-Agent\": \"MyWikipediaScraper/1.0 (contact@example.com)\"  # Replace with your actual email\n",
        "    }\n",
        "\n",
        "    # Parameters for searching articles related to the topic\n",
        "    search_params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": topic,\n",
        "        \"format\": \"json\",\n",
        "        \"srlimit\": 5  # Limit to 5 articles\n",
        "    }\n",
        "\n",
        "    # Make the search request\n",
        "    search_response = requests.get(search_url, headers=headers, params=search_params)\n",
        "    search_data = search_response.json()\n",
        "\n",
        "    # Dictionary to store titles and content of articles\n",
        "    articles = {}\n",
        "\n",
        "    # Loop over search results and fetch content for each article\n",
        "    for result in search_data[\"query\"][\"search\"]:\n",
        "        title = result[\"title\"]\n",
        "\n",
        "        # Parameters for fetching the page content\n",
        "        content_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"prop\": \"extracts\",\n",
        "            \"explaintext\": True,\n",
        "            \"titles\": title,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "\n",
        "        # Make the request for article content\n",
        "        content_response = requests.get(search_url, headers=headers, params=content_params)\n",
        "        content_data = content_response.json()\n",
        "\n",
        "        # Extract page content\n",
        "        page = next(iter(content_data[\"query\"][\"pages\"].values()))\n",
        "        if \"extract\" in page:\n",
        "            articles[title] = page[\"extract\"]  # Store the title and content\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Example usage\n",
        "topic = \"Artificial Intelligence\"\n",
        "articles = get_wikipedia_articles(topic)\n",
        "\n",
        "# Print article titles and a preview of their content\n",
        "for title, content in articles.items():\n",
        "    print(f\"Title: {title}\\nContent Preview: {content[:1000]}...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BASXutLXQ9L2",
        "outputId": "22f655b5-fbf9-4962-81e1-278ba755901d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Artificial intelligence\n",
            "Content Preview: Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\n",
            "High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applica...\n",
            "\n",
            "Title: Artificial general intelligence\n",
            "Content Preview: Artificial general intelligence (AGI) is a type of artificial intelligence (AI) that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks. This contrasts with narrow AI, which is limited to specific tasks. Artificial superintelligence (ASI), on the other hand, refers to AGI that greatly exceeds human cognitive capabilities. AGI is considered one of the definitions of strong AI.\n",
            "Creating AGI is a primary goal of AI research and of companies such as OpenAI, Google, and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\n",
            "The timeline for achieving AGI remains a subject of ongoing debate among researchers and experts. As of 2023, some argue that it may be possible in years or decades; others maintain it might take a century or longer; a minority believe it may never be achieved; and another minority claims that it is already here. Notable AI researcher Geoffrey Hinton has expressed concerns about the rapid ...\n",
            "\n",
            "Title: A.I. Artificial Intelligence\n",
            "Content Preview: A.I. Artificial Intelligence (or simply A.I.) is a 2001 American science fiction film directed by Steven Spielberg. The screenplay by Spielberg and screen story by Ian Watson are loosely based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. Set in a futuristic society, the film stars Haley Joel Osment as David, a childlike android uniquely programmed with the ability to love. Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt star in supporting roles.\n",
            "Development of A.I. originally began after producer and director Stanley Kubrick acquired the rights to Aldiss's story in the early 1970s. Kubrick hired a series of writers, including Aldiss, Bob Shaw, Ian Watson and Sara Maitland, until the mid-1990s. The film languished in development hell for years, partly because Kubrick felt that computer-generated imagery was not advanced enough to create the David character, which he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I....\n",
            "\n",
            "Title: Applications of artificial intelligence\n",
            "Content Preview: Artificial intelligence (AI) has been used in applications throughout industry and academia. In a manner analogous to electricity or computers, AI serves as a general-purpose technology. AI programs are designed to simulate human perception and understanding. These systems are capable of adapting to new information and responding to changing situations. Machine learning has been used for various scientific and commercial purposes including language translation, image recognition, decision-making, credit scoring, and e-commerce.\n",
            "\n",
            "\n",
            "== Internet and e-commerce ==\n",
            "\n",
            "\n",
            "=== Web feeds and posts ===\n",
            "Machine learning has been used for recommendation systems in determining which posts should show up in social media feeds. Various types of social media analysis also make use of machine learning and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.\n",
            "AI has been used to customize shopping options and personalize offer...\n",
            "\n",
            "Title: Generative artificial intelligence\n",
            "Content Preview: Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \n",
            "Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\n",
            "Generative AI has uses across a wide range of industries, including software development, heal...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data cleaning"
      ],
      "metadata": {
        "id": "vlTrZhDtRTKi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTm4_RJ8Qm7x",
        "outputId": "0f2cd88b-6f57-436e-d9cc-4f08e2cba965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text: Artificial intelligence AI is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by animals including humans. Leading AI textbooks define the field as the study of \"intelligent agents\": any system that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def remove_references(text):\n",
        "    \"\"\"\n",
        "    Removes reference tags (e.g., [1], [citation needed]) from Wikipedia text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text containing references.\n",
        "\n",
        "    Returns:\n",
        "    str: Cleaned text without references.\n",
        "    \"\"\"\n",
        "    # Remove patterns like [1], [12], [citation needed]\n",
        "    return re.sub(r'\\[.*?\\]', '', text)\n",
        "\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    \"\"\"\n",
        "    Removes special characters like newline and tab characters from text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text to clean.\n",
        "\n",
        "    Returns:\n",
        "    str: Cleaned text without special characters.\n",
        "    \"\"\"\n",
        "    # Replace newlines and tabs with spaces\n",
        "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
        "\n",
        "    # Remove other special characters, if any (you can add more as needed)\n",
        "    text = re.sub(r'[^A-Za-z0-9.,;:!?\\'\" ]+', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize_whitespace(text):\n",
        "    \"\"\"\n",
        "    Normalizes whitespace by replacing multiple spaces with a single space.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text with extra spaces.\n",
        "\n",
        "    Returns:\n",
        "    str: Text with normalized whitespace.\n",
        "    \"\"\"\n",
        "    # Replace multiple spaces with a single space\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans Wikipedia text by removing references, special characters, and normalizing whitespace.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The raw Wikipedia article text.\n",
        "\n",
        "    Returns:\n",
        "    str: Fully cleaned text.\n",
        "    \"\"\"\n",
        "    text = remove_references(text)\n",
        "    text = remove_special_characters(text)\n",
        "    text = normalize_whitespace(text)\n",
        "    return text\n",
        "\n",
        "# Example Usage\n",
        "raw_text = \"\"\"\n",
        "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by animals including humans.[1] Leading AI textbooks define the field as the study of \"intelligent agents\"[citation needed]: any system that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n",
        "\"\"\"\n",
        "\n",
        "# Clean the text using our utility function\n",
        "cleaned_text = clean_text(raw_text)\n",
        "print(\"Cleaned Text:\", cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import sklearn\n",
        "\n",
        "\n",
        "nltk.__version__, sklearn.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDfRItytRl9f",
        "outputId": "961a5d03-f50e-48b3-eaf8-cb3895e5b9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('3.9.1', '1.6.1')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "\n",
        "# nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfKPWch_RchR",
        "outputId": "44ebc607-e211-4719-dead-a7283678bcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_article = list(articles.keys())[0]\n",
        "cleaned_text = clean_text(articles[first_article])\n",
        "\n",
        "sentences = sent_tokenize(cleaned_text)\n",
        "sentences[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oUe5ibhRwad",
        "outputId": "607653e7-7ac2-47ce-fc15-c4d741448620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Artificial intelligence AI refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problemsolving, perception, and decisionmaking.',\n",
              " 'It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBBN9ulGRx60",
        "outputId": "5b7b26eb-2e56-4765-f5a0-eef2386f86d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "575"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's create embeddings (vectors)\n",
        "\n",
        "TF-IDF assigns a score to each word based on its importance. By converting each sentence to a TF-IDF vector, we can find the sentences with the highest scores, which are more likely to represent the main points."
      ],
      "metadata": {
        "id": "Zuyu0THSSJiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TfidfVectorizer with English stop words\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Compute TF-IDF scores for each sentence\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Display TF-IDF matrix shape to understand the output\n",
        "print(\"TF-IDF Matrix shape:\", tfidf_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpq7K2XRSGPb",
        "outputId": "52fe3ae4-0769-4245-c425-1d2d32658610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix shape: (575, 2987)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(tfidf_matrix.sum(axis=1)).flatten()[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_uQXl0FSOq6",
        "outputId": "15eebdf5-2082-4303-8ee5-360e49598162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.98990294, 4.50122519, 1.69971014, 5.71521362, 3.50986723])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization using TF-IDF scores\n",
        "\n",
        "Here we rank sentences by their importance in the article.\n",
        "\n",
        "To summarize, we want to select the top sentences that represent the text. We can use the sum of TF-IDF scores for each sentence as a measure of importance."
      ],
      "metadata": {
        "id": "Ow6az9oRo_2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum the TF-IDF scores for each sentence (row)\n",
        "sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
        "\n",
        "# Get the indices of sentences sorted by score (descending order)\n",
        "top_sentence_indices = sentence_scores.argsort()[-3:][::-1]\n",
        "\n",
        "# Display the top-ranked sentence indices\n",
        "print(\"Top sentence indices:\", top_sentence_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46AzmWFLSRoK",
        "outputId": "d4043398-ea39-4c17-855d-b9d468bcecd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top sentence indices: [560 153 398]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap"
      ],
      "metadata": {
        "id": "K4g91Bu3SUsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's build a Summary"
      ],
      "metadata": {
        "id": "ZiHjFtlXSidi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the top sentences for the summary\n",
        "summary_sentences = [sentences[i] for i in top_sentence_indices]\n",
        "\n",
        "# Join the sentences to form the summary text\n",
        "summary = ' '.join(summary_sentences)\n",
        "\n",
        "wrapped_summary = textwrap.fill(summary, width=90)\n",
        "print(\"Summary:\")\n",
        "print(wrapped_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTv2prBeSaqT",
        "outputId": "f66407c5-72d0-48e5-b742-4a10b9c560f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "See also Artificial intelligence and elections Use and impact of AI on political elections\n",
            "Artificial intelligence content detection Software to detect AIgenerated content Behavior\n",
            "selection algorithm Algorithm that selects actions for intelligent agents Business process\n",
            "automation Automation of business processes Casebased reasoning Process of solving new\n",
            "problems based on the solutions of similar past problems Computational intelligence\n",
            "Ability of a computer to learn a specific task from data or experimental observation\n",
            "Digital immortality Hypothetical concept of storing a personality in digital form Emergent\n",
            "algorithm Algorithm exhibiting emergent behavior Female gendering of AI technologies\n",
            "Gender biases in digital technologyPages displaying short descriptions of redirect targets\n",
            "Glossary of artificial intelligence List of definitions of terms and concepts commonly\n",
            "used in the study of artificial intelligence Intelligence amplification Use of information\n",
            "technology to augment human intelligence Intelligent agent Software agent which acts\n",
            "autonomously Mind uploading Hypothetical process of digitally emulating a brain Organoid\n",
            "intelligence Use of brain cells and brain organoids for intelligent computing Robotic\n",
            "process automation Form of business process automation technology The Last Day novel 1967\n",
            "Welsh science fiction novel Welsh science novel by Owain Owain Wetware computer Computer\n",
            "composed of organic material Explanatory notes References AI textbooks The two most widely\n",
            "used textbooks in 2023 see the Open Syllabus: Russell, Stuart J.; Norvig, Peter 2021.\n",
            "Applications AI and machine learning technology is used in most of the essential\n",
            "applications of the 2020s, including: search engines such as Google Search, targeting\n",
            "online advertisements, recommendation systems offered by Netflix, YouTube or Amazon,\n",
            "driving internet traffic, targeted advertising AdSense, Facebook, virtual assistants such\n",
            "as Siri or Alexa, autonomous vehicles including drones, ADAS and selfdriving cars,\n",
            "automatic language translation Microsoft Translator, Google Translate, facial recognition\n",
            "Apple's Face ID or Microsoft's DeepFace and Google's FaceNet and image labeling used by\n",
            "Facebook, Apple's iPhoto and TikTok. An AI framework such as the Care and Act Framework\n",
            "containing the SUM valuesdeveloped by the Alan Turing Institute tests projects in four\n",
            "main areas: Respect the dignity of individual people Connect with other people sincerely,\n",
            "openly, and inclusively Care for the wellbeing of everyone Protect social values, justice,\n",
            "and the public interest Other developments in ethical frameworks include those decided\n",
            "upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the\n",
            "IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles\n",
            "are not without criticism, especially regards to the people chosen to contribute to these\n",
            "frameworks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem with TF-IDF approach - Non grammatical summary\n",
        "\n",
        "The issue with extraction-based summarization producing non-grammatical summaries often stems from the approach of selecting top sentences based solely on TF-IDF scores. High TF-IDF scores don’t necessarily ensure that the sentences will flow naturally when put together, as they were designed to be read in context."
      ],
      "metadata": {
        "id": "iVfglwPvpyN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "JPOjXKpJqtD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some production grade tooling - TextRank\n",
        "\n",
        "TextRank is a graph-based algorithm that ranks sentences based on similarity scores. TextRank can produce more readable and meaningful summaries by choosing sentences that best represent the text collectively.\n",
        "\n",
        "<br>\n",
        "\n",
        "Pros of Using TextRank for Extractive Summarization\n",
        "- Maintains Readability: TextRank considers sentence similarity, which leads to better sentence cohesion.\n",
        "\n",
        "- Concise yet Coherent Summaries: TextRank captures the central idea while avoiding redundant sentences."
      ],
      "metadata": {
        "id": "czAOVHSLS12J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sumy\n",
        "\n",
        "!pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzdrSoeoSknD",
        "outputId": "63c02fe9-fa9e-483a-eb2a-735bb65d1c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.1.31)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=9b8900ec18d0893e026c26a7dcb7786103ccb9faa1332b26a3a5001997b17dc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=556ab948c7e2c9cdf2ffba769475619f920544beaf0d944e97fd08b59bd0376b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sumy\n",
        "\n",
        "sumy.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iObsBby7SrDp",
        "outputId": "59398c12-027f-4db4-d094-444a55babfed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.11.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "def extractive_summary_textrank(text, sentence_count=3):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary of the text using the TextRank algorithm from Sumy.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text to summarize.\n",
        "    sentence_count (int): The number of sentences to include in the summary.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated summary.\n",
        "    \"\"\"\n",
        "    # Create a parser for the input text\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "\n",
        "    # Initialize the TextRank summarizer\n",
        "    summarizer = TextRankSummarizer()\n",
        "\n",
        "    # Generate summary with the specified number of sentences\n",
        "    summary = summarizer(parser.document, sentence_count)\n",
        "\n",
        "    # Join the summarized sentences into a single string\n",
        "    summary_text = \" \".join([str(sentence) for sentence in summary])\n",
        "\n",
        "    return summary_text"
      ],
      "metadata": {
        "id": "ft3zFJEYSv7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the summary with 3 sentences\n",
        "summary = extractive_summary_textrank(cleaned_text, sentence_count=3)\n",
        "\n",
        "wrapped_summary = textwrap.fill(summary, width=90)\n",
        "print(\"Summary:\")\n",
        "print(wrapped_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa3f6IksSwwC",
        "outputId": "bbf33bcf-b1b7-4c2d-aec0-702647325bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "The emergence of advanced generative AI in the midst of the AI boom and its ability to\n",
            "create and modify content exposed several unintended consequences and harms in the present\n",
            "and raised concerns about the risks of AI and its longterm effects in the future,\n",
            "prompting discussions about regulatory policies to ensure the safety and benefits of the\n",
            "technology. An AI framework such as the Care and Act Framework containing the SUM\n",
            "valuesdeveloped by the Alan Turing Institute tests projects in four main areas: Respect\n",
            "the dignity of individual people Connect with other people sincerely, openly, and\n",
            "inclusively Care for the wellbeing of everyone Protect social values, justice, and the\n",
            "public interest Other developments in ethical frameworks include those decided upon during\n",
            "the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's\n",
            "Ethics of Autonomous Systems initiative, among others; however, these principles are not\n",
            "without criticism, especially regards to the people chosen to contribute to these\n",
            "frameworks. See also Artificial intelligence and elections Use and impact of AI on\n",
            "political elections Artificial intelligence content detection Software to detect\n",
            "AIgenerated content Behavior selection algorithm Algorithm that selects actions for\n",
            "intelligent agents Business process automation Automation of business processes Casebased\n",
            "reasoning Process of solving new problems based on the solutions of similar past problems\n",
            "Computational intelligence Ability of a computer to learn a specific task from data or\n",
            "experimental observation Digital immortality Hypothetical concept of storing a personality\n",
            "in digital form Emergent algorithm Algorithm exhibiting emergent behavior Female gendering\n",
            "of AI technologies Gender biases in digital technologyPages displaying short descriptions\n",
            "of redirect targets Glossary of artificial intelligence List of definitions of terms and\n",
            "concepts commonly used in the study of artificial intelligence Intelligence amplification\n",
            "Use of information technology to augment human intelligence Intelligent agent Software\n",
            "agent which acts autonomously Mind uploading Hypothetical process of digitally emulating a\n",
            "brain Organoid intelligence Use of brain cells and brain organoids for intelligent\n",
            "computing Robotic process automation Form of business process automation technology The\n",
            "Last Day novel 1967 Welsh science fiction novel Welsh science novel by Owain Owain Wetware\n",
            "computer Computer composed of organic material Explanatory notes References AI textbooks\n",
            "The two most widely used textbooks in 2023 see the Open Syllabus: Russell, Stuart J.;\n",
            "Norvig, Peter 2021.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Theory time !\n",
        "\n",
        "#### What is TF-IDF?\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a technique to evaluate the importance of a word in a document relative to a collection of documents (corpus).\n",
        "\n",
        "1. Term Frequency (TF): Measures how frequently a word appears in a document. The more frequent the word, the higher its TF score for that document.\n",
        "\n",
        "2. Inverse Document Frequency (IDF): Measures how unique a word is across all documents in the corpus. Common words like \"the\" and \"is\" are less informative, so they get a lower IDF score, while unique terms get higher scores.\n",
        "\n",
        "\n",
        "TF-IDF combines these two metrics by calculating:\n",
        "\n",
        "  `TF-IDF = TF × IDF`\n",
        "\n",
        "\n",
        "Words with high TF-IDF scores in a document are considered more important for that document, making TF-IDF useful for identifying key words and phrases within texts.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### TF-IDF vs. TextRank in Extractive Summarization\n",
        "\n",
        "In extractive summarization, TF-IDF can help by identifying the most informative words within a document. A common approach is to:\n",
        "\n",
        "- Compute the TF-IDF scores of words,\n",
        "\n",
        "- Identify sentences that contain the highest scoring words,\n",
        "\n",
        "- Extract these top sentences to form a summary.\n",
        "\n",
        "However, this approach often produces disjointed summaries that may not be grammatically coherent, as it doesn't consider sentence-to-sentence relationships. This is where TextRank provides an advantage.\n",
        "\n",
        "---\n",
        "\n",
        "#### TextRank: `A Graph-Based Summarization Algorithm`\n",
        "\n",
        "TextRank doesn’t directly use TF-IDF. Instead, it builds a graph of sentence relationships based on semantic similarity. Here’s how it works:\n",
        "\n",
        "1. Convert Text into Sentences: Split the text into individual sentences.\n",
        "\n",
        "2. Build Sentence Graph: Treat each sentence as a node in a graph. Then, connect sentences (nodes) by edges if they have a high similarity score. Similarity can be calculated using various methods, like cosine similarity between sentence embeddings (e.g., with Word2Vec or BERT embeddings).\n",
        "\n",
        "3. Rank Sentences: TextRank ranks sentences based on how many similar sentences link to them and how important those linking sentences are. The result is a measure of centrality, where highly-ranked sentences are central to the text’s meaning.\n",
        "\n",
        "4. Extract Top Sentences: Finally, extract the top-ranked sentences as the summary.\n",
        "\n",
        "By capturing sentence relationships, TextRank produces summaries that are often more coherent and grammatically correct than a TF-IDF approach.\n",
        "\n",
        "\n",
        "#### Why We Used TextRank Instead of TF-IDF for Summarization\n",
        "\n",
        "In our case, Sumy’s TextRank summarizer provided better coherence and readability by selecting sentences based on their centrality in the document rather than just their term importance, as TF-IDF would.\n",
        "\n",
        "---\n",
        "\n",
        "TextRank is better suited for extractive summarization because:\n",
        "\n",
        "It maintains grammatical coherence by considering sentence-to-sentence similarity.\n",
        "It selects sentences that are representative of the entire document’s content."
      ],
      "metadata": {
        "id": "Pjom4K-8rPSg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ep-HWaASzJZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
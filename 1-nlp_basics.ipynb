{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "Breaking down a text into individual components (like words or sentences).\n",
        "\n",
        "For example, tokenizing “The quick brown fox” would yield [“The”, “quick”, “brown”, “fox”]."
      ],
      "metadata": {
        "id": "wxhvP4PUlVAL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiMYRiUok8xx",
        "outputId": "4ebbf202-110b-4d21-a203-71a9d9f18959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# nltk.download(\"punkt\") # NLTK Punkt tokenizer for sentence splitting\n",
        "nltk.download(\"punkt_tab\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zUFoA-XGnaaL",
        "outputId": "a5d58ba9-3f62-4d16-e5cf-38916f4084d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.9.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Tokenization"
      ],
      "metadata": {
        "id": "BcBc2YBdl-NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The small boy ate the pancakes from the restaurant\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "print(f'Tokens using word tokenize {tokens}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA_zzfeemBzb",
        "outputId": "c6ebb22f-c617-40f5-a196-2584412bc60f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens using word tokenize ['The', 'small', 'boy', 'ate', 'the', 'pancakes', 'from', 'the', 'restaurant']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Tokenization"
      ],
      "metadata": {
        "id": "ADuE-JWnmTlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article = '''Did you know that World War 1 brought German Shepherds to the Western Hemisphere? Rin Tin Tin was a German Shepherd rescued from WW1. They are some of the smartest dogs!'''\n",
        "\n",
        "sentence_tokens = nltk.sent_tokenize(article)\n",
        "\n",
        "print(f'Tokens using sent tokenize {sentence_tokens}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDjD0Wz2mVca",
        "outputId": "6524d691-70ea-4e32-9128-a67f9ac592af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens using sent tokenize ['Did you know that World War 1 brought German Shepherds to the Western Hemisphere?', 'Rin Tin Tin was a German Shepherd rescued from WW1.', 'They are some of the smartest dogs!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# More examples\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "paragraph = \"The small boy ate the pancakes from the restaurant. He didn't liked them, and the next thing he wanted to try is pizza. But that's too costly!\"\n",
        "\n",
        "print(\"Word tokenizer\", word_tokenize(paragraph), end=\"\\n\\n\")\n",
        "\n",
        "print(\"Sentence tokenizer\", sent_tokenize(paragraph))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNiUv6HWn585",
        "outputId": "de62cdd7-46ea-4490-c500-f1a16ff54293"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenizer ['The', 'small', 'boy', 'ate', 'the', 'pancakes', 'from', 'the', 'restaurant', '.', 'He', 'did', \"n't\", 'liked', 'them', ',', 'and', 'the', 'next', 'thing', 'he', 'wanted', 'to', 'try', 'is', 'pizza', '.', 'But', 'that', \"'s\", 'too', 'costly', '!']\n",
            "\n",
            "Sentence tokenizer ['The small boy ate the pancakes from the restaurant.', \"He didn't liked them, and the next thing he wanted to try is pizza.\", \"But that's too costly!\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "FOEBDt5KotaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part of Speech (POS) Tagging\n",
        "\n",
        "Assigning grammatical labels to words, such as identifying “quick” as an adjective and “fox” as a noun."
      ],
      "metadata": {
        "id": "AeTYh9pYo0gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm1NC_4iofTR",
        "outputId": "ef2ec192-0bd9-4b41-e443-05532f072a11"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW7qDtL2o5Af",
        "outputId": "087176d2-cecc-4ccf-c41d-d12695887eb4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'small', 'boy', 'ate', 'the', 'pancakes', 'from', 'the', 'restaurant']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags = nltk.pos_tag(tokens)\n",
        "\n",
        "tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sT8ZJTipDXe",
        "outputId": "7c7476c3-c7c7-4429-a0e1-b84cf729f5b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('small', 'JJ'),\n",
              " ('boy', 'NN'),\n",
              " ('ate', 'VBP'),\n",
              " ('the', 'DT'),\n",
              " ('pancakes', 'NNS'),\n",
              " ('from', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('restaurant', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "\n",
        "secret = \"I am from United States, and you ?\"\n",
        "\n",
        "secret_tokens = nltk.word_tokenize(secret)\n",
        "\n",
        "secret_tagged_tokens = pos_tag(secret_tokens)\n",
        "\n",
        "secret_tagged_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46W1Xk5KpJyu",
        "outputId": "907df4d8-fdea-424d-a999-fbab380b2b93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('from', 'IN'),\n",
              " ('United', 'NNP'),\n",
              " ('States', 'NNPS'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('you', 'PRP'),\n",
              " ('?', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Parse Tree"
      ],
      "metadata": {
        "id": "RBrFWGQbty78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3XFaNqUt51M",
        "outputId": "8dc90b7b-1899-47b0-f7e5-43a85f4a6834"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "chunker = RegexpParser(\"\"\"\n",
        "  NP: {<DT>?<JJ>*<NN>} #To extract the Noun Phrases\n",
        "  P: {<IN>} #To extract Prepositions\n",
        "  V: {<V.*>} #To extract Verbs\n",
        "  PP: {<P> <NP>} #To extract Prepositional Phrases\n",
        "  VP: {<V> <NP|PP>*} #To extract Verb Phrases\n",
        "\"\"\")\n",
        "\n",
        "output = chunker.parse(secret_tagged_tokens)\n",
        "\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "nmrMcoN6qAR0",
        "outputId": "09aa72f7-c7d7-415e-f5aa-4acef1184ebc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [('I', 'PRP'), Tree('VP', [Tree('V', [('am', 'VBP')])]), Tree('P', [('from', 'IN')]), ('United', 'NNP'), ('States', 'NNPS'), (',', ','), ('and', 'CC'), ('you', 'PRP'), ('?', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"216px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,384.0,216.0\" width=\"384px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"10.4167%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.20833%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4167%\" x=\"10.4167%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">am</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"12.5%\" x=\"20.8333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">P</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.0833%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16.6667%\" x=\"33.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">United</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"16.6667%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">States</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNPS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.3333%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.25%\" x=\"66.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.7917%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4167%\" x=\"72.9167%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4167%\" x=\"83.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">you</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.5417%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.25%\" x=\"93.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">?</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.875%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Additional resources\n",
        "\n",
        "1. [NLTK chunking docs](https://www.nltk.org/howto/chunk.html)\n",
        "\n",
        "2. [IBM Watson resource for POS tagging](https://www.ibm.com/docs/en/wca/3.5.0?topic=analytics-part-speech-tag-sets)"
      ],
      "metadata": {
        "id": "sCcCdMD5upmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "IFcWoTlDvQ0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization and Stemming\n",
        "\n",
        "Reducing words to their root forms, so “running” and “ran” become “run.”"
      ],
      "metadata": {
        "id": "ihp344b1vSC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `Stemming` is the process of reducing a word to its root form by chopping off prefixes or suffixes.\n",
        "\n",
        "  - Stemming Example: \"running\" → \"run\"\n",
        "\n",
        "---\n",
        "\n",
        "2. `Lemmatization` is the process of converting a word to its base or dictionary form, considering its meaning and context.\n",
        "\n",
        "  - Lemmatization Example: \"better\" → \"good\""
      ],
      "metadata": {
        "id": "8P4v5EjmwOLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "BsNiQZtI1xRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMyZjzWqvRY3",
        "outputId": "ec0e8e05-2a27-424b-c43f-d1c15f5d4b30"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "token = \"stockings\"\n",
        "\n",
        "result_lemma = lemmatizer.lemmatize(token)\n",
        "\n",
        "print(token, \"=>\", result_lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55bsnSjzxTix",
        "outputId": "1b48c945-dae9-415d-e974-7e1bf891157a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stockings => stocking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Let's lemmatize an entire sentence now"
      ],
      "metadata": {
        "id": "mcKEJ8-ax_AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # for tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPB3U-g8zqpv",
        "outputId": "a9d0f25e-34d1-4af7-de40-fd23d7bc098e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"I am running to the store because I needed to buy some groceries.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokenized sentence:\")\n",
        "print(tokens)\n",
        "\n",
        "# Lemmatize the tokenized sentences\n",
        "lemmatized_tokens = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
        "print(\"Lemmatized sentence:\")\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwC33GKoxawA",
        "outputId": "5c86f83b-7eb8-4930-bd46-35838b7b74dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentence:\n",
            "['I', 'am', 'running', 'to', 'the', 'store', 'because', 'I', 'needed', 'to', 'buy', 'some', 'groceries', '.']\n",
            "Lemmatized sentence:\n",
            "I am running to the store because I needed to buy some grocery .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CfTr4azH09-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am running to the store because I needed to buy some groceries.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "lemmatized_words = [\n",
        "    lemmatizer.lemmatize(word, pos='v') if word != \"I\" else word for word in tokens\n",
        "]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9qaAf0u1Poz",
        "outputId": "cb2be191-393f-49e5-ae12-a38a3371e5f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words: ['I', 'be', 'run', 'to', 'the', 'store', 'because', 'I', 'need', 'to', 'buy', 'some', 'groceries', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In lemmatization, we can specify the part of speech (POS) for **better accuracy**; in the above example, I specified `pos='v'` for verbs."
      ],
      "metadata": {
        "id": "FmFQ0nA62sLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "1yXOUONu12yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"I am running to the store because I needed to buy some groceries.\"\n",
        "\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xzeb0A0zlTa",
        "outputId": "f0a50a9f-b932-4bd7-d016-0d41bde76cea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['i', 'am', 'run', 'to', 'the', 'store', 'becaus', 'i', 'need', 'to', 'buy', 'some', 'groceri', '.']\n"
          ]
        }
      ]
    }
  ]
}